{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from simpleDNN import SimpleDNN\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights=[10264.0,5151.0,5128.0,5069.0,\n",
    "#  4998.0,\n",
    "#  4935.0,\n",
    "#  4909.0,\n",
    "#  4901.0,\n",
    "#  4785.0,\n",
    "#  4613.0,\n",
    "#  4601.0,\n",
    "#  4393.0,\n",
    "#  4376.0,\n",
    "#  4305.0,\n",
    "#  4120.0,\n",
    "#  4083.0,\n",
    "#  4061.0,\n",
    "#  4019.0,\n",
    "#  3906.0,\n",
    "#  3891.0,\n",
    "#  3817.0,\n",
    "#  3599.0,\n",
    "#  3431.0,\n",
    "#  3405.0,\n",
    "#  3397.0,\n",
    "#  3361.0,\n",
    "#  3349.0,\n",
    "#  3327.0,\n",
    "#  3319.0,\n",
    "#  3232.0,\n",
    "#  3190.0,\n",
    "#  3028.0,\n",
    "#  3018.0,\n",
    "#  2896.0,\n",
    "#  2892.0,\n",
    "#  2759.0,\n",
    "#  2715.0,\n",
    "#  2613.0,\n",
    "#  2587.0,\n",
    "#  2550.0,\n",
    "#  2398.0,\n",
    "#  2304.0,\n",
    "#  2228.0,\n",
    "#  2094.0,\n",
    "#  2063.0,\n",
    "#  1930.0,\n",
    "#  1192.0,\n",
    "#  345.0]\n",
    "# weights-np.array(weights)\n",
    "# weights=weights/np.max(weights)\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file:str, features:list):\n",
    "        # Load the data\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data['class_label'] = np.argmax(self.data[['label_proton', 'label_pion']].values, axis=1)\n",
    "        self.data = self.data.drop(['label_proton', 'label_pion'], axis=1)\n",
    "        self.data = self.data.dropna()\n",
    "        self.X = self.data[features].values\n",
    "        # self.X = StandardScaler().fit_transform(self.X)\n",
    "        self.X = QuantileTransformer().fit_transform(self.X)\n",
    "        self.y = self.data['class_label'].values \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return one sample of data\n",
    "        X_sample = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y_sample = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return X_sample, y_sample\n",
    "\n",
    "# Step 2: Use DataLoader\n",
    "\n",
    "def create_dataloader(csv_file, batch_size=32, shuffle=True,num=1,train=0.6,valid=0.2,test=0.2,features=[]):\n",
    "    dataset = CustomDataset(csv_file, features=features)\n",
    "   \n",
    "    indices = np.arange(len(dataset))\n",
    "    np.random.shuffle(indices)\n",
    "    train=int(train*len(dataset))\n",
    "    valid=int(valid*len(dataset))\n",
    "    test=int(test*len(dataset))\n",
    "    print(f\"Train: {train}, Valid: {valid}, Test: {test}\")\n",
    "    train_indices,valid_indices,test_indices = indices[:train], indices[train:valid+train],indices[train+valid:]\n",
    "    print(f\"Train: {len(train_indices)}, Valid: {len(valid_indices)}, Test: {len(test_indices)}\")   \n",
    "    train_loader=DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_indices),num_workers=num)\n",
    "    valid_loader=DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(valid_indices),num_workers=num)\n",
    "    test_loader=DataLoader(dataset, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_indices),num_workers=num)\n",
    "    return train_loader,valid_loader,test_loader\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    \n",
    "    train_loss_ep = 0.\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm.tqdm(train_loader, ascii=True) as tq:\n",
    "        for data, target in tq:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            # loss = F.nll_loss(output, target)\n",
    "            loss= F.cross_entropy(output, target) # It is not recommended to use cross entropy loss for Binary classification.\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            train_loss_ep += loss.item() * data.size(0)\n",
    "        \n",
    "    return train_loss_ep\n",
    "\n",
    "def test(model, device, valid_loader1):\n",
    "    \n",
    "    test_loss_ep = 0.\n",
    "    model.eval()\n",
    "    with tqdm.tqdm(valid_loader1, ascii=True) as tq:\n",
    "        for data, target in tq:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # loss = F.nll_loss(output, target)\n",
    "            loss= F.cross_entropy(output, target)\n",
    "            \n",
    "            test_loss_ep += loss.item() * data.size(0)\n",
    "        \n",
    "    return test_loss_ep\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_file = \"/mnt/tsv/smeared_50_50_50/pp_smeared_50_50_50.csv\"  # Replace with your actual CSV file\n",
    "f=['speed',  \n",
    " 'AsymmetryY',\n",
    " 'centralTowerFraction_cell',\n",
    " 'AsymmetryX',\n",
    " 'VarianceAtVertex',\n",
    " 'weightedTime',\n",
    " 'AsymmetryX_plain',\n",
    " 'EfractionCell',\n",
    " 'AsymmetryY_plain',\n",
    " 'TotalEnergy',\n",
    " 'numCellBeforeVertex',\n",
    " 'radialSigma',\n",
    " 'Asymmetry',\n",
    " 'RatioEcell',\n",
    " 'PostVertexEnergyFraction',\n",
    " 'Asymmetry_plain',\n",
    " 'longitudinalSigma',\n",
    " 'longitudinalSigma_plain',\n",
    " 'radius',\n",
    " 'DeltaEcell_secondMax',\n",
    " 'NumberOfUniqueCells',\n",
    " 'TotalEnergyCloseToVertex',\n",
    " 'radialSigma_plain',\n",
    " 'length',\n",
    " 'theta2',\n",
    " 'Z_vertex',\n",
    " 'd2',\n",
    " 'MaxEnergyInCell',\n",
    " 'MaxEnergyCloseVertex',\n",
    " 'E2',\n",
    " 'distanceMaxFromVertex',\n",
    " 'EnergyFractionCloseToVertex',\n",
    " 'radius_plain',\n",
    " 'E1',\n",
    " 'length_plain',\n",
    " 'SecondMaxEnergyInCell',\n",
    " 'R1',\n",
    " 'VertexTime',\n",
    " 'Aplanarity',\n",
    " 'R2',\n",
    " 'distanceFirstSecondMaxEnergy',\n",
    " 'DeltaT',\n",
    " 'E3',\n",
    " 'theta3',\n",
    " 'd3',\n",
    " 'R3',\n",
    " 'time0',\n",
    " 'NumPeaks']\n",
    "#f lis a list of features to be used in the model (you can add or reove time0 here)\n",
    "\n",
    "train_loader,valid_loader,test_loader = create_dataloader(csv_file, batch_size=128,num=28,features=f)\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape[1], y_batch.shape)\n",
    "    input_size = X_batch.shape[1]\n",
    "    break\n",
    "hidden_layers = [96,32,16]  # Example of hidden layer sizes\n",
    "model = SimpleDNN(input_size, hidden_layers) # I also tried to use weights=weights in the model but it did not work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "n_epochs = 200\n",
    "loss_data = []\n",
    "\n",
    "valid_loss_min = np.inf  # set initial \"min\" to infinity\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "  \n",
    "    weight_decay=0.001\n",
    "    lr=0.0009\n",
    "    lr=lr/(1+weight_decay*(epoch)**2.5)\n",
    "    if epoch%10==0:\n",
    "        lr=0.00005\n",
    "    print(f\" LR: {np.round(lr,6)}\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)  \n",
    "    # optimizer = optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay,momentum=0.9)  #SGD optimizer is pretty slow compared to Adam\n",
    "    train_loss = train(model, device, train_loader, optimizer)\n",
    "    valid_loss = test(model, device, valid_loader)\n",
    "    \n",
    "    \n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "   \n",
    "\n",
    "    loss_data.append({'Epoch': epoch + 1, 'Training Loss': train_loss, 'Validation Loss': valid_loss})\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_hcal.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    loss_history_df = pd.DataFrame(loss_data)\n",
    "    loss_history_df.to_csv('loss_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = SimpleDNN(input_size, hidden_layers)\n",
    "model_test.load_state_dict(torch.load('model_hcal.pt',weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval() # Set model to eval mode\n",
    "    true_preds, num_preds = 0., 0.\n",
    "    \n",
    "    with torch.no_grad(): # Deactivate gradients for the following code\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "            \n",
    "           \n",
    "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            \n",
    "          \n",
    "            pred_labels = torch.argmax(preds, dim=-1) # Binarize predictions to 0 and 10\n",
    "            \n",
    "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
    "            true_preds += (pred_labels == data_labels).sum()\n",
    "            num_preds += data_labels.shape[0]\n",
    "            \n",
    "    acc = true_preds / num_preds\n",
    "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model_test, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader):\n",
    "    model.eval() # Set model to eval mode\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad(): # Deactivate gradients for the following code\n",
    "        for data_inputs, data_labels in data_loader:\n",
    "\n",
    "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
    "            preds = model(data_inputs)\n",
    "            pred_labels.extend(torch.argmax(preds, dim=-1).cpu().numpy()) # Binarize predictions to 0 and 10\n",
    "            true_labels.extend(data_labels.cpu().numpy())\n",
    "            \n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=['proton', 'pion'], yticklabels=['proton', 'pion'])\n",
    "    plt.xticks(weight='bold',size=12)\n",
    "    plt.yticks(weight='bold',size=12)\n",
    "    plt.xlabel(\"Prediction\",weight=\"bold\",size=12)\n",
    "    plt.ylabel('Truth',weight='bold',size=12)\n",
    "    plt.title('Confusion Matrix',weight='bold',size=15)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(model_test, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
